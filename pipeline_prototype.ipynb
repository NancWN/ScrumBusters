{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not ready to be used for anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.7.1\n",
      "TFX version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\Meine Ablage\\\\TechLabs\\\\Scam Busters'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "# should be .../Scam Busters or whatever you called this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the path of the data directory\n",
    "data_root = '/dir/ect/ory'\n",
    "# path of the UDF module\n",
    "# the UDF module contains functions which are used by the transform and train components\n",
    "module_file = '/path/module.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"busters\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "pipeline_root = os.path.join('pipelines', pipeline_name)\n",
    "\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "metadata_path = os.path.join('metadata', pipeline_name, 'metadata.db')\n",
    "\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "serving_model_dir = os.path.join('serving_model', pipeline_name)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# imports for tfx and tf\n",
    "from tfx.components import StatisticsGen\n",
    "\n",
    "# csv will be replaced by parquet for better performance\n",
    "from tfx.components import CsvExampleGen\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.components import Transform\n",
    "from tfx.components import Trainer\n",
    "from tfx.components import Evaluator\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import Pusher\n",
    "\n",
    "from tfx.dsl.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "\n",
    "from tfx.dsl.components.common import resolver\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from tfx.types.standard_artifacts import Model\n",
    "from tfx.types.standard_artifacts import ModelBlessing\n",
    "\n",
    "from tfx.types import Channel\n",
    "\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(b/137289334): rename this as simple after DAG visualization is done.\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str,\n",
    "                     metadata_path: str,\n",
    "                     beam_pipeline_args: List[str]) -> pipeline.Pipeline:\n",
    "\n",
    "    # staging\n",
    "    # time to load the dataset\n",
    "\n",
    "    # csv will be replaced by parquet for better performance\n",
    "    example_gen = CsvExampleGen(input_base='data_root')\n",
    "\n",
    "    # two options, unsure which one is better\n",
    "    # pre-transform data to have one datapoint per row\n",
    "    # or load in multiple files to clena up in the transformer\n",
    "    # thoughts on pre-transform: we plan to have a dataset generator to create the single data points\n",
    "    # we have to make pre-transform trackable, tfx cannot do that (as far as I know)\n",
    "    # statistics generator\n",
    "    stats_gen = StatisticsGen(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        name='compute-eval-stats'\n",
    "        )\n",
    "    # schema generation\n",
    "    # it should be checked if there's an existing, curated schema\n",
    "    # if there is, it should be used\n",
    "    # the check will be implemented later\n",
    "    schema_gen = tfx.components.SchemaGen(\n",
    "        statistics=stats_gen.outputs['statistics'])\n",
    "    # Performs anomaly detection based on statistics and data schema.\n",
    "    validate_stats = ExampleValidator(\n",
    "        statistics=stats_gen.outputs['statistics'],\n",
    "        schema=schema_gen.outputs['schema'])\n",
    "    # Performs transformations and feature engineering in training and serving.\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        module_file=module_file)\n",
    "    # module file refers to user defined functions that are used by transformer & trainer\n",
    "    # Uses user-provided Python function that implements a model.\n",
    "    trainer = Trainer(\n",
    "        module_file=module_file,\n",
    "        custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
    "        eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n",
    "    # as far as i understand this takes the recently trained model\n",
    "    # only purpose is to put the latest model into the validator\n",
    "\n",
    "    # Get the latest blessed model for model validation.\n",
    "    model_resolver = resolver.Resolver(\n",
    "        strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
    "        model=Channel(type=Model),\n",
    "        model_blessing=Channel(\n",
    "            type=ModelBlessing)).with_id('latest_blessed_model_resolver')\n",
    "    # Uses TFMA to compute a evaluation statistics over features of a model and\n",
    "    # perform quality validation of a candidate model (compared to a baseline).\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='tips')],\n",
    "        slicing_specs=[tfma.SlicingSpec()],\n",
    "        metrics_specs=[\n",
    "            tfma.MetricsSpec(metrics=[\n",
    "                tfma.MetricConfig(\n",
    "                    class_name='BinaryAccuracy',\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            lower_bound={'value': 0.6}),\n",
    "                        change_threshold=tfma.GenericChangeThreshold(\n",
    "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                            absolute={'value': -1e-10})))\n",
    "            ])\n",
    "        ])\n",
    "\n",
    "    model_analyzer = Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        # Change threshold will be ignored if there is no baseline (first run).\n",
    "        eval_config=eval_config)\n",
    "    # Checks whether the model passed the validation steps and pushes the model\n",
    "    # to a file destination if check passed.\n",
    "    pusher = Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        model_blessing=model_analyzer.outputs['blessing'],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                base_directory=serving_model_dir)))\n",
    "\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=[\n",
    "            example_gen,\n",
    "            stats_gen,\n",
    "            schema_gen,\n",
    "            validate_stats,\n",
    "            transform,\n",
    "            trainer,\n",
    "            model_resolver,\n",
    "            model_analyzer,\n",
    "            pusher,\n",
    "        ],\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "            metadata_path),\n",
    "        beam_pipeline_args=beam_pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11736\\3996111140.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m       \u001b[0mpipeline_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpipeline_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m       \u001b[0mpipeline_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpipeline_root\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m       \u001b[0mdata_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m       \u001b[0mschema_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m       \u001b[0mmodule_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodule_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_root' is not defined"
     ]
    }
   ],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      data_root=data_root,\n",
    "      # schema path will be used if curated schemas are available\n",
    "      # the code in _create_pipeline will be adapted to accept schema paths and check if they need to be created\n",
    "      # schema_path=schema_path,\n",
    "      module_file=module_file,\n",
    "      serving_model_dir=serving_model_dir,\n",
    "      metadata_path=metadata_path,))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65dee9a9d1a923d78ab16c84a55db276e0248384890ce461db8ba4ddd71fba4b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('busters')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
