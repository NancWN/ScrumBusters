{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not ready to be used for anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.7.1\n",
      "TFX version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\Meine Ablage\\\\TechLabs\\\\Scam Busters'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "# should be .../Scam Busters or whatever you called this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the path of the data directory\n",
    "data_root = 'data\\\\engineered\\\\'\n",
    "# path of the UDF module\n",
    "# the UDF module contains functions which are used by the transform and train components\n",
    "module_file = 'module.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"busters\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "pipeline_root = os.path.join('pipelines', pipeline_name)\n",
    "\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "metadata_path = os.path.join('.','metadata', pipeline_name, 'metadata.db')\n",
    "\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "serving_model_dir = os.path.join('serving_model', pipeline_name)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\metadata\\\\busters\\\\metadata.db'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# imports for tfx and tf\n",
    "from tfx.components import StatisticsGen\n",
    "\n",
    "# csv will be replaced by parquet for better performance\n",
    "from tfx.components import CsvExampleGen\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.components import Transform\n",
    "from tfx.components import Trainer\n",
    "from tfx.components import Evaluator\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import Pusher\n",
    "\n",
    "from tfx.dsl.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "\n",
    "from tfx.dsl.components.common import resolver\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from tfx.types.standard_artifacts import Model\n",
    "from tfx.types.standard_artifacts import ModelBlessing\n",
    "\n",
    "from tfx.types import Channel\n",
    "\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(b/137289334): rename this as simple after DAG visualization is done.\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str,\n",
    "                     metadata_path: str,\n",
    "                     beam_pipeline_args: List[str] = None) -> pipeline.Pipeline:\n",
    "\n",
    "    # staging\n",
    "    # time to load the dataset\n",
    "\n",
    "    # csv will be replaced by parquet for better performance\n",
    "    example_gen = CsvExampleGen(input_base=data_root)\n",
    "\n",
    "    # two options, unsure which one is better\n",
    "    # pre-transform data to have one datapoint per row\n",
    "    # or load in multiple files to clena up in the transformer\n",
    "    # thoughts on pre-transform: we plan to have a dataset generator to create the single data points\n",
    "    # we have to make pre-transform trackable, tfx cannot do that (as far as I know)\n",
    "    # statistics generator\n",
    "    stats_gen = StatisticsGen(\n",
    "        examples=example_gen.outputs['examples']\n",
    "        # ,name='compute-eval-stats'\n",
    "        )\n",
    "    # schema generation\n",
    "    # it should be checked if there's an existing, curated schema\n",
    "    # if there is, it should be used\n",
    "    # the check will be implemented later\n",
    "    schema_gen = tfx.components.SchemaGen(\n",
    "        statistics=stats_gen.outputs['statistics'])\n",
    "    # Performs anomaly detection based on statistics and data schema.\n",
    "    validate_stats = ExampleValidator(\n",
    "        statistics=stats_gen.outputs['statistics'],\n",
    "        schema=schema_gen.outputs['schema'])\n",
    "    # Performs transformations and feature engineering in training and serving.\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        module_file=module_file)\n",
    "    # module file refers to user defined functions that are used by transformer & trainer\n",
    "    # Uses user-provided Python function that implements a model.\n",
    "    trainer = Trainer(\n",
    "        module_file=module_file,\n",
    "        # custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
    "        eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n",
    "    # as far as i understand this takes the recently trained model\n",
    "    # only purpose is to put the latest model into the validator\n",
    "\n",
    "    # Get the latest blessed model for model validation.\n",
    "    model_resolver = resolver.Resolver(\n",
    "        strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
    "        model=Channel(type=Model),\n",
    "        model_blessing=Channel(\n",
    "            type=ModelBlessing)).with_id('latest_blessed_model_resolver')\n",
    "    # Uses TFMA to compute a evaluation statistics over features of a model and\n",
    "    # perform quality validation of a candidate model (compared to a baseline).\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='tips')],\n",
    "        slicing_specs=[tfma.SlicingSpec()],\n",
    "        metrics_specs=[\n",
    "            tfma.MetricsSpec(metrics=[\n",
    "                tfma.MetricConfig(\n",
    "                    class_name='BinaryAccuracy',\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            lower_bound={'value': 0.6}),\n",
    "                        change_threshold=tfma.GenericChangeThreshold(\n",
    "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                            absolute={'value': -1e-10})))\n",
    "            ])\n",
    "        ])\n",
    "\n",
    "    model_analyzer = Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        # Change threshold will be ignored if there is no baseline (first run).\n",
    "        eval_config=eval_config)\n",
    "    # Checks whether the model passed the validation steps and pushes the model\n",
    "    # to a file destination if check passed.\n",
    "    pusher = Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        model_blessing=model_analyzer.outputs['blessing'],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                base_directory=serving_model_dir)))\n",
    "\n",
    "    components=[\n",
    "            example_gen,\n",
    "            stats_gen,\n",
    "            schema_gen,\n",
    "            validate_stats,\n",
    "            transform,\n",
    "            trainer,\n",
    "            model_resolver,\n",
    "            model_analyzer,\n",
    "            pusher,\n",
    "        ]\n",
    "\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=sqlite_metadata_connection_config(metadata_path),\n",
    "        # metadata.sqlite_metadata_connection_config(metadata_path),\n",
    "        beam_pipeline_args=beam_pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.v1.dsl.io import fileio\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "\n",
    "def sqlite_metadata_connection_config(\n",
    "    metadata_db_uri: str) -> metadata_store_pb2.ConnectionConfig:\n",
    "    \"\"\"Convenience function to create file based metadata connection config.\n",
    "\n",
    "    Args:\n",
    "        metadata_db_uri: uri to metadata db.\n",
    "\n",
    "    Returns:\n",
    "        A metadata_store_pb2.ConnectionConfig based on given metadata db uri.\n",
    "    \"\"\"\n",
    "    fileio.makedirs(os.path.dirname(metadata_db_uri))\n",
    "    connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "    connection_config.sqlite.filename_uri = metadata_db_uri\n",
    "    connection_config.sqlite.connection_mode = (\n",
    "        metadata_store_pb2.SqliteMetadataSourceConfig.READWRITE_OPENCREATE)\n",
    "    return connection_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlite {\n",
       "  filename_uri: \".\\\\metadata\\\\busters\\\\metadata.db\"\n",
       "  connection_mode: READWRITE_OPENCREATE\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlite_metadata_connection_config(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n"
     ]
    }
   ],
   "source": [
    "pipeline_test = _create_pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      data_root=data_root,\n",
    "      # schema path will be used if curated schemas are available\n",
    "      # the code in _create_pipeline will be adapted to accept schema paths and check if they need to be created\n",
    "      # schema_path=schema_path,\n",
    "      module_file=module_file,\n",
    "      serving_model_dir=serving_model_dir,\n",
    "      metadata_path=metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Generating ephemeral wheel package for 'g:\\\\Meine Ablage\\\\TechLabs\\\\Scam Busters\\\\module.py' (including modules: ['module']).\n",
      "INFO:absl:User module package has hash fingerprint version 8fc706649011f203c41364ea9577cc328ce401fb285fe72408ab0ceeddf8a877.\n",
      "INFO:absl:Executing: ['C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\pipeline\\\\python.exe', 'C:\\\\Users\\\\SNKE~1\\\\AppData\\\\Local\\\\Temp\\\\tmp86vw676s\\\\_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', 'C:\\\\Users\\\\SNKE~1\\\\AppData\\\\Local\\\\Temp\\\\tmp3ddzd3y2', '--dist-dir', 'C:\\\\Users\\\\SNKE~1\\\\AppData\\\\Local\\\\Temp\\\\tmpcdyhrkck']\n",
      "INFO:absl:Successfully built user code wheel distribution at 'pipelines\\\\busters\\\\_wheels\\\\tfx_user_code_Transform-0.0+8fc706649011f203c41364ea9577cc328ce401fb285fe72408ab0ceeddf8a877-py3-none-any.whl'; target user module is 'module'.\n",
      "INFO:absl:Full user module path is 'module@pipelines\\\\busters\\\\_wheels\\\\tfx_user_code_Transform-0.0+8fc706649011f203c41364ea9577cc328ce401fb285fe72408ab0ceeddf8a877-py3-none-any.whl'\n",
      "INFO:absl:Generating ephemeral wheel package for 'g:\\\\Meine Ablage\\\\TechLabs\\\\Scam Busters\\\\module.py' (including modules: ['module']).\n",
      "INFO:absl:User module package has hash fingerprint version 8fc706649011f203c41364ea9577cc328ce401fb285fe72408ab0ceeddf8a877.\n",
      "INFO:absl:Executing: ['C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\pipeline\\\\python.exe', 'C:\\\\Users\\\\SNKE~1\\\\AppData\\\\Local\\\\Temp\\\\tmp2ywkf_f3\\\\_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', 'C:\\\\Users\\\\SNKE~1\\\\AppData\\\\Local\\\\Temp\\\\tmp197asmy6', '--dist-dir', 'C:\\\\Users\\\\SNKE~1\\\\AppData\\\\Local\\\\Temp\\\\tmp54xurofp']\n",
      "INFO:absl:Successfully built user code wheel distribution at 'pipelines\\\\busters\\\\_wheels\\\\tfx_user_code_Trainer-0.0+8fc706649011f203c41364ea9577cc328ce401fb285fe72408ab0ceeddf8a877-py3-none-any.whl'; target user module is 'module'.\n",
      "INFO:absl:Full user module path is 'module@pipelines\\\\busters\\\\_wheels\\\\tfx_user_code_Trainer-0.0+8fc706649011f203c41364ea9577cc328ce401fb285fe72408ab0ceeddf8a877-py3-none-any.whl'\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Evaluator\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ExampleValidator\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_validator.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Pusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"SchemaGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.schema_gen.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"StatisticsGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Trainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Transform\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.transform.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \".\\\\metadata\\\\busters\\\\metadata.db\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \".\\\\metadata\\\\busters\\\\metadata.db\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"busters\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-01T16:14:33.580644\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"busters.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"data\\\\engineered\\\\\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"Transform\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:mlmd client InternalError: Error when executing query: disk I/O error query: COMMIT;\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Error when executing query: disk I/O error query: COMMIT;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12804\\1751071610.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m tfx.orchestration.LocalDagRunner().run(\n\u001b[0m\u001b[0;32m      2\u001b[0m   pipeline_test)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pipeline\\lib\\site-packages\\tfx\\orchestration\\portable\\tfx_runner.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, pipeline, run_options)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m       \u001b[0mrun_options_pb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_with_ir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline_pb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_options_pb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pipeline\\lib\\site-packages\\tfx\\orchestration\\local\\local_dag_runner.py\u001b[0m in \u001b[0;36mrun_with_ir\u001b[1;34m(self, pipeline, run_options)\u001b[0m\n\u001b[0;32m    107\u001b[0m           \u001b[1;32mwith\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection_config\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmlmd_handle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mpartial_run_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnapshot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlmd_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mcomponent_launcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Component %s is finished.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pipeline\\lib\\site-packages\\tfx\\orchestration\\portable\\launcher.py\u001b[0m in \u001b[0;36mlaunch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m     \u001b[1;31m# Runs as a normal node.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m     \u001b[0mexecution_preparation_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m     (execution_info, contexts,\n\u001b[0;32m    526\u001b[0m      \u001b[0mis_execution_needed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexecution_preparation_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pipeline\\lib\\site-packages\\tfx\\orchestration\\portable\\launcher.py\u001b[0m in \u001b[0;36m_prepare_execution\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mlmd_connection\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m       \u001b[1;31m# 1.Prepares all contexts.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m       contexts = context_lib.prepare_contexts(\n\u001b[0m\u001b[0;32m    257\u001b[0m           metadata_handler=m, node_contexts=self._pipeline_node.contexts)\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pipeline\\lib\\site-packages\\tfx\\orchestration\\portable\\mlmd\\context_lib.py\u001b[0m in \u001b[0;36mprepare_contexts\u001b[1;34m(metadata_handler, node_contexts)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmetadata_store_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContext\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m   \"\"\"\n\u001b[1;32m--> 151\u001b[1;33m   return [\n\u001b[0m\u001b[0;32m    152\u001b[0m       _register_context_if_not_exist(\n\u001b[0;32m    153\u001b[0m           metadata_handler=metadata_handler, context_spec=context_spec)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pipeline\\lib\\site-packages\\tfx\\orchestration\\portable\\mlmd\\context_lib.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    150\u001b[0m   \"\"\"\n\u001b[0;32m    151\u001b[0m   return [\n\u001b[1;32m--> 152\u001b[1;33m       _register_context_if_not_exist(\n\u001b[0m\u001b[0;32m    153\u001b[0m           metadata_handler=metadata_handler, context_spec=context_spec)\n\u001b[0;32m    154\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mcontext_spec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_contexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pipeline\\lib\\site-packages\\tfx\\orchestration\\portable\\mlmd\\context_lib.py\u001b[0m in \u001b[0;36m_register_context_if_not_exist\u001b[1;34m(metadata_handler, context_spec)\u001b[0m\n\u001b[0;32m     92\u001b[0m       metadata_handler=metadata_handler, context_spec=context_spec)\n\u001b[0;32m     93\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mcontext_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput_contexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m   \u001b[1;31m# This might happen in cases we have parallel executions of nodes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pipeline\\lib\\site-packages\\ml_metadata\\metadata_store\\metadata_store.py\u001b[0m in \u001b[0;36mput_contexts\u001b[1;34m(self, contexts)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata_store_service_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPutContextsResponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PutContexts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pipeline\\lib\\site-packages\\ml_metadata\\metadata_store\\metadata_store.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, method_name, request, response)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAbortedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mnum_retries\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pipeline\\lib\\site-packages\\ml_metadata\\metadata_store\\metadata_store.py\u001b[0m in \u001b[0;36m_call_method\u001b[1;34m(self, method_name, request, response)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_using_db_connection\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m       \u001b[0mcc_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadata_store_serialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_cc_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m       \u001b[0mgrpc_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata_store_stub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pipeline\\lib\\site-packages\\ml_metadata\\metadata_store\\metadata_store.py\u001b[0m in \u001b[0;36m_pywrap_cc_call\u001b[1;34m(self, method, request, response)\u001b[0m\n\u001b[0;32m    234\u001b[0m         self._metadata_store, request.SerializeToString())\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0m_make_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m     \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Error when executing query: disk I/O error query: COMMIT;"
     ]
    }
   ],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  pipeline_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65dee9a9d1a923d78ab16c84a55db276e0248384890ce461db8ba4ddd71fba4b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('busters')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
